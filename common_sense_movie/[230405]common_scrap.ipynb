{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import selenium.common.exceptions as sex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "import random\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "start_url = \"https://www.commonsensemedia.org/movie-lists\"\n",
    "driver.get(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down():\n",
    "    elem = driver.find_element_by_tag_name(\"body\")\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            elem.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # print(\"끝\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down():\n",
    "    elem = driver.find_element(By.TAG_NAME,\"body\")\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            elem.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # print(\"끝\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_title = []\n",
    "# link_list = []\n",
    "\n",
    "scroll_down()\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "for i in soup.find_all('div', class_=\"col\"):\n",
    "    title = i.find('a')\n",
    "    move_title.append(title.text)\n",
    "    k = i.find('a', class_=\"link link--title\")\n",
    "    text = str(k)\n",
    "    text = text.split('href=\"')[-1]\n",
    "    link = text.split('\">')[0]\n",
    "    link = \"https://www.commonsensemedia.org\"+link\n",
    "    link_list.append(link)\n",
    "\n",
    "print(len(move_title))\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(move_title))\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = move_title[1:]\n",
    "link_list = link_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title':movie_title,'link':link_list})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cat_title_link.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finding Nemo</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despicable Me</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doug</td>\n",
       "      <td>https://www.commonsensemedia.org/tv-reviews/doug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encanto</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                                               link\n",
       "0   Finding Nemo  https://www.commonsensemedia.org/movie-reviews...\n",
       "1         Frozen  https://www.commonsensemedia.org/movie-reviews...\n",
       "2  Despicable Me  https://www.commonsensemedia.org/movie-reviews...\n",
       "3           Doug   https://www.commonsensemedia.org/tv-reviews/doug\n",
       "4        Encanto  https://www.commonsensemedia.org/movie-reviews..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_title_v2 = []\n",
    "link_list_v2 = []\n",
    "\n",
    "\n",
    "for num, link in enumerate(df.link.tolist()):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        scroll_down()\n",
    "        driver.implicitly_wait(2)\n",
    "        page = driver.page_source\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        for i in soup.find_all('div', class_=\"col\"):\n",
    "            title = i.find('a')\n",
    "            move_title_v2.append(title.text)\n",
    "            k = i.find('a', class_=\"link link--title\")\n",
    "            text = str(k)\n",
    "            text = text.split('href=\"')[-1]\n",
    "            link = text.split('\">')[0]\n",
    "            link = \"https://www.commonsensemedia.org\"+link\n",
    "            link_list_v2.append(link)\n",
    "        print('남음', len(df.link.tolist())-num)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "move_title_v2 = move_title_v2[1:]\n",
    "link_list_v2 = link_list_v2[1:]\n",
    "\n",
    "print(len(move_title_v2))\n",
    "print(len(link_list_v2))\n",
    "\n",
    "df2 = pd.DataFrame({'title':move_title_v2,'link':link_list_v2})\n",
    "df2.head()\n",
    "\n",
    "\n",
    "df2 = df2.to_csv('title_link_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finding Nemo</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despicable Me</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doug</td>\n",
       "      <td>https://www.commonsensemedia.org/tv-reviews/doug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encanto</td>\n",
       "      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                                               link\n",
       "0   Finding Nemo  https://www.commonsensemedia.org/movie-reviews...\n",
       "1         Frozen  https://www.commonsensemedia.org/movie-reviews...\n",
       "2  Despicable Me  https://www.commonsensemedia.org/movie-reviews...\n",
       "3           Doug   https://www.commonsensemedia.org/tv-reviews/doug\n",
       "4        Encanto  https://www.commonsensemedia.org/movie-reviews..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Text_Mining\\\\common_sense_movie\\\\title_link_v2.csv\", index_col=0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Documents\\Monicas_Workspace\\child_text\\reviews\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\user\\Documents\\Monicas_Workspace\\child_text\\reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "\n",
    "for num, link in enumerate(df2.link.tolist()):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(3)\n",
    "        page = driver.page_source\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        \n",
    "\n",
    "        k =soup.find('div', class_='review-view-summary row')\n",
    "        k2 = k.find('div', class_='col-12')\n",
    "        title = k2.find('h1').text\n",
    "        text_title = str(title)\n",
    "        move_title_v3.append(text_title)\n",
    "        f = open('{}_reviews.txt'.format(text_title),'w')\n",
    "        \n",
    "        ## see the full review\n",
    "        button = driver.find_element(By.XPATH,r'//*//*[@id=\"content\"]/div/div[2]/div/div/div[1]/review-view-summary/div/div[4]/div[1]/div[2]/p/a').click()\n",
    "        driver.implicitly_wait(3)\n",
    "\n",
    "        ## counting reviews\n",
    "        cnt = soup.find_all('div', class_=\"col-6\")\n",
    "        ch_cnt = cnt[1:]\n",
    "        for i in ch_cnt:\n",
    "            cnt = i.find('a', class_=\"link link--cta\")\n",
    "            cont = str(cnt.text).strip().split(' ')[0]\n",
    "            cont = int(cont)\n",
    "            \n",
    "        try:\n",
    "            for i in range(cont):\n",
    "                page = driver.page_source\n",
    "                soup = BeautifulSoup(page, 'html.parser')\n",
    "                scroll_down()\n",
    "\n",
    "                ages = soup.find_all('span',class_='user-summary__title')\n",
    "                for i in ages:\n",
    "                    age = i.text\n",
    "                    age = age.replace('\\n','')\n",
    "                    \n",
    "       \n",
    "\n",
    "                content = soup.find_all('div', class_='reveal__content')\n",
    "                for i in content:\n",
    "                    reviews = i.text\n",
    "                    reviews = reviews.strip()\n",
    "                    f.write('++++++++++++++++++++++++++++++++'+age+'+++++++++++++++++++++++++++++\\n')\n",
    "                    f.write('-'+reviews+'\\n')\n",
    "\n",
    "                \n",
    "\n",
    "                driver.find_element(By.CSS_SELECTOR, \"[aria-label='Goto next page']\").click()\n",
    "            f.close()\n",
    "        except:\n",
    "            f.close()\n",
    "            pass\n",
    "    \n",
    "        k+=1\n",
    "        print(len(df2.link.tolist())-k, '남음')\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teen, 13 years old\n",
      "Teen, 17 years old\n",
      "Teen, 14 years old\n",
      "Teen, 17 years old\n",
      "Teen, 16 years old\n",
      "Kid, 9 years old\n",
      "Teen, 16 years old\n",
      "Teen, 16 years old\n",
      "Teen, 17 years old\n",
      "Kid, 11 years old\n"
     ]
    }
   ],
   "source": [
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "ages = soup.find_all('span',class_='user-summary__title')\n",
    "for i in ages:\n",
    "    age = i.text\n",
    "    age = age.replace('\\n','')\n",
    "    print(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f1eceaeee7ce272db888c3a7c4a53fa68076ef98b13c09fb04dc1844b9fc9a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
