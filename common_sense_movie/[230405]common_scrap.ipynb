{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import selenium.common.exceptions as sex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "import random\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\AppData\\Local\\Temp/ipykernel_32636/3853711423.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "start_url = \"https://www.commonsensemedia.org/movie-lists\"\n",
    "driver.get(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down():\n",
    "    elem = driver.find_element_by_tag_name(\"body\")\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            elem.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # print(\"끝\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\AppData\\Local\\Temp/ipykernel_51096/163869049.py:2: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  elem = driver.find_element_by_tag_name(\"body\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝\n"
     ]
    }
   ],
   "source": [
    "scroll_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32636/3193570824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element(By.XPATH,r'//*//*[@id=\"pagination-2ea8469d-8238-45b9-99cc-2d88e6b58022\"]/ul/li[{}]/a'.format(k)).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\AppData\\Local\\Temp/ipykernel_32636/163869049.py:2: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  elem = driver.find_element_by_tag_name(\"body\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝\n",
      "214\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "# move_title = []\n",
    "# link_list = []\n",
    "\n",
    "scroll_down()\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "for i in soup.find_all('div', class_=\"col\"):\n",
    "    title = i.find('a')\n",
    "    move_title.append(title.text)\n",
    "    k = i.find('a', class_=\"link link--title\")\n",
    "    text = str(k)\n",
    "    text = text.split('href=\"')[-1]\n",
    "    link = text.split('\">')[0]\n",
    "    link = \"https://www.commonsensemedia.org\"+link\n",
    "    link_list.append(link)\n",
    "\n",
    "print(len(move_title))\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "print(len(move_title))\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = move_title[1:]\n",
    "link_list = link_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kids' Choice Awards: All-Time Stars</td>\n",
       "      <td>https://www.commonsensemedia.org/lists/kids-ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023 Oscar-Nominated Films</td>\n",
       "      <td>https://www.commonsensemedia.org/lists/2023-os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023 NAACP Image Award-Nominated Films</td>\n",
       "      <td>https://www.commonsensemedia.org/lists/2023-na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50 Modern Movies All Kids Should Watch Before ...</td>\n",
       "      <td>https://www.commonsensemedia.org/lists/50-mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Powerful Stories About Martin Luther King Jr.</td>\n",
       "      <td>https://www.commonsensemedia.org/lists/powerfu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                Kids' Choice Awards: All-Time Stars   \n",
       "1                         2023 Oscar-Nominated Films   \n",
       "2             2023 NAACP Image Award-Nominated Films   \n",
       "3  50 Modern Movies All Kids Should Watch Before ...   \n",
       "4      Powerful Stories About Martin Luther King Jr.   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.commonsensemedia.org/lists/kids-ch...  \n",
       "1  https://www.commonsensemedia.org/lists/2023-os...  \n",
       "2  https://www.commonsensemedia.org/lists/2023-na...  \n",
       "3  https://www.commonsensemedia.org/lists/50-mode...  \n",
       "4  https://www.commonsensemedia.org/lists/powerfu...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'title':movie_title,'link':link_list})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cat_title_link.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\AppData\\Local\\Temp/ipykernel_32636/1186869078.py:2: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  elem = driver.find_element_by_tag_name(\"body\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남음 213\n",
      "남음 212\n",
      "남음 211\n",
      "남음 210\n",
      "남음 209\n",
      "남음 208\n",
      "남음 207\n",
      "남음 206\n",
      "남음 205\n",
      "남음 204\n",
      "남음 203\n",
      "남음 202\n",
      "남음 201\n",
      "남음 200\n",
      "남음 199\n",
      "남음 198\n",
      "남음 197\n",
      "남음 196\n",
      "남음 195\n",
      "남음 194\n",
      "남음 192\n",
      "남음 191\n",
      "남음 190\n",
      "남음 189\n",
      "남음 188\n",
      "남음 187\n",
      "남음 186\n",
      "남음 185\n",
      "남음 184\n",
      "남음 183\n",
      "남음 182\n",
      "남음 181\n",
      "남음 180\n",
      "남음 179\n",
      "남음 178\n",
      "남음 177\n",
      "남음 176\n",
      "남음 175\n",
      "남음 174\n",
      "남음 173\n",
      "남음 171\n",
      "남음 170\n",
      "남음 169\n",
      "남음 168\n",
      "남음 167\n",
      "남음 166\n",
      "남음 165\n",
      "남음 164\n",
      "남음 163\n",
      "남음 161\n",
      "남음 160\n",
      "남음 159\n",
      "남음 158\n",
      "남음 157\n",
      "남음 156\n",
      "남음 155\n",
      "남음 154\n",
      "남음 153\n",
      "남음 152\n",
      "남음 150\n",
      "남음 149\n",
      "남음 148\n",
      "남음 147\n",
      "남음 146\n",
      "남음 145\n",
      "남음 144\n",
      "남음 143\n",
      "남음 142\n",
      "남음 141\n",
      "남음 140\n",
      "남음 139\n",
      "남음 138\n",
      "남음 137\n",
      "남음 136\n",
      "남음 135\n",
      "남음 134\n",
      "남음 133\n",
      "남음 132\n",
      "남음 131\n",
      "남음 129\n",
      "남음 128\n",
      "남음 127\n",
      "남음 126\n",
      "남음 125\n",
      "남음 124\n",
      "남음 123\n",
      "남음 122\n",
      "남음 121\n",
      "남음 120\n",
      "남음 119\n",
      "남음 118\n",
      "남음 117\n",
      "남음 116\n",
      "남음 115\n",
      "남음 114\n",
      "남음 113\n",
      "남음 112\n",
      "남음 111\n"
     ]
    }
   ],
   "source": [
    "move_title_v2 = []\n",
    "link_list_v2 = []\n",
    "\n",
    "\n",
    "for num, link in enumerate(df.link.tolist()):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        scroll_down()\n",
    "        driver.implicitly_wait(3)\n",
    "        page = driver.page_source\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        for i in soup.find_all('div', class_=\"col\"):\n",
    "            title = i.find('a')\n",
    "            move_title_v2.append(title.text)\n",
    "            k = i.find('a', class_=\"link link--title\")\n",
    "            text = str(k)\n",
    "            text = text.split('href=\"')[-1]\n",
    "            link = text.split('\">')[0]\n",
    "            link = \"https://www.commonsensemedia.org\"+link\n",
    "            link_list_v2.append(link)\n",
    "        print('남음', len(df.link.tolist())-num)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "move_title_v2 = move_title_v2[1:]\n",
    "link_list_v2 = link_list_v2[1:]\n",
    "\n",
    "print(len(move_title_v2))\n",
    "print(len(link_list_v2))\n",
    "\n",
    "df2 = pd.DataFrame({'title':move_title_v2,'link':link_list_v2})\n",
    "df2.head()\n",
    "\n",
    "\n",
    "df = df.to_csv('title_link_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_title_v3 = []\n",
    "cnt_list_v3 = []\n",
    "\n",
    "for num, link in enumerate(df2.link.tolist()):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(3)\n",
    "        page = driver.page_source\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        k =soup.find('div', class_='review-view-summary row')\n",
    "        k2 = k.find('div', class_='col-12')\n",
    "        title = k2.find('h1').text\n",
    "        text_title = str(title)\n",
    "        move_title_v3.append(text_title)\n",
    "\n",
    "        button = driver.find_element(By.XPATH,r'//*//*[@id=\"content\"]/div/div[2]/div/div/div[1]/review-view-summary/div/div[4]/div[1]/div[2]/p/a').click()\n",
    "        driver.implicitly_wait(3)\n",
    "        \n",
    "        \n",
    "        cnt = soup.find_all('div', class_=\"col-6\")\n",
    "        ch_cnt = cnt[1:]\n",
    "        for i in ch_cnt:\n",
    "            cnt = i.find('a', class_=\"link link--cta\")\n",
    "            cont = str(cnt.text).strip().split(' ')[0]\n",
    "            cnt_list_v3.append(cont)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(move_title_v3))\n",
    "print(len(cnt_list_v3))\n",
    "\n",
    "df3 = pd.DataFrame({'title':move_title_v3,'cnt':cnt_list_v3})\n",
    "df3.head()      \n",
    "\n",
    "df3.to_csv('title_review_cnt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('reviews.txt','w')\n",
    "# move_title_v3 = []\n",
    "# link_list_v3 = []\n",
    "\n",
    "# for num, link in enumerate(tqdm(df2.link.tolist()[:2])):\n",
    "\n",
    "#     f.write('------------------------------------------')\n",
    "    \n",
    "#     driver.get(link)\n",
    "#     driver.implicitly_wait(3)\n",
    "#     page = driver.page_source\n",
    "#     soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "#     k =soup.find('div', class_='review-view-summary row')\n",
    "#     k2 = k.find('div', class_='col-12')\n",
    "#     title = k2.find('h1').text\n",
    "#     text_title = str(title)\n",
    "#     f.write(text_title)\n",
    "\n",
    "#     button = driver.find_element(By.XPATH,r'//*//*[@id=\"content\"]/div/div[2]/div/div/div[1]/review-view-summary/div/div[4]/div[1]/div[2]/p/a').click()\n",
    "#     driver.implicitly_wait(3)\n",
    "#     button = driver.find_element(By.XPATH,r'//*//*[@id=\"content\"]/div/div[2]/div/div/div[2]/user-reviews-summary/div/div[3]/div/div[2]/p/a').click()\n",
    "#     scroll_down()\n",
    "\n",
    "#     page = driver.page_source\n",
    "#     soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "#     content = soup.find_all('div', class_='reveal__content')\n",
    "\n",
    "#     for i in content:\n",
    "#         reviews = i.text\n",
    "#         reviews = reviews.strip()\n",
    "#         f.write('-'+reviews+'\\n')\n",
    "\n",
    "#     try:\n",
    "        \n",
    "#         button = driver.find_element(By.XPATH,r'//*//*[@id=\"pagination-a9c3cede-229c-48a2-9352-f7b6cd75f046\"]/ul/li[11]/a').click()\n",
    "#         scroll_down()\n",
    "\n",
    "#         page = driver.page_source\n",
    "#         soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "#         content = soup.find_all('div', class_='reveal__content')\n",
    "\n",
    "#         for i in content:\n",
    "#             reviews = i.text\n",
    "#             reviews = reviews.strip()\n",
    "#             f.write('-'+reviews+'\\n')\n",
    "#     except:\n",
    "#         button = driver.find_element(By.XPATH,r'//*//*[@id=\"pagination-54843471-1fdf-4350-868d-75a60d2a5e48\"]/ul/li[12]/a').click()\n",
    "#         scroll_down()\n",
    "\n",
    "#         page = driver.page_source\n",
    "#         soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "#         content = soup.find_all('div', class_='reveal__content')\n",
    "\n",
    "#         for i in content:\n",
    "#             reviews = i.text\n",
    "#             reviews = reviews.strip()\n",
    "#             f.write('-'+reviews+'\\n')\n",
    "    \n",
    "#     else:\n",
    "#         button = driver.find_element(By.XPATH,r'//*//*[@id=\"pagination-54843471-1fdf-4350-868d-75a60d2a5e48\"]/ul/li[13]/a').click()\n",
    "#         scroll_down()\n",
    "\n",
    "#         page = driver.page_source\n",
    "#         soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "#         content = soup.find_all('div', class_='reveal__content')\n",
    "\n",
    "#         for i in content:\n",
    "#             reviews = i.text\n",
    "#             reviews = reviews.strip()\n",
    "#             f.write('-'+reviews+'\\n')\n",
    "\n",
    "\n",
    "# f.close()\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\yukir\\\\Documents\\\\GitHub\\\\Text_Mining\\\\common_sense_movie'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//*[@id=\"pagination-a9c3cede-229c-48a2-9352-f7b6cd75f046\"]/ul/li[11]/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457\n"
     ]
    }
   ],
   "source": [
    "cnt = soup.find_all('div', class_=\"col-6\")\n",
    "ch_cnt = cnt[1:]\n",
    "for i in ch_cnt:\n",
    "    cnt = i.find('a', class_=\"link link--cta\")\n",
    "    print(str(cnt.text).strip().split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "cnt = soup.find('a', class_=\"link link--cta\")\n",
    "print(str(cnt.text).strip().split(' ')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Dory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\AppData\\Local\\Temp/ipykernel_32636/163869049.py:2: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  elem = driver.find_element_by_tag_name(\"body\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝\n",
      "- A delightful sequel promotes teamwork; some sadness scares.\n",
      "- I rate finding dory for rated U\n",
      "- Funny and fully enchanting sequel to hit film.\n",
      "- FISH DONT SLEEP\n",
      "- Great movie. Extremely appropriate\n",
      "- Finding Dory is a sweet movie about the importance of family. Dory sets out to find her parents. I is very sweet. There is also the importance of friendship. Sadly, it isn't as great as the first, but still amazing, WATCH IT!\n"
     ]
    }
   ],
   "source": [
    "k =soup.find('div', class_='review-view-summary row')\n",
    "k2 = k.find('div', class_='col-12')\n",
    "title = k2.find('h1').text\n",
    "print(title)\n",
    "button = driver.find_element(By.XPATH,r'//*//*[@id=\"content\"]/div/div[2]/div/div/div[1]/review-view-summary/div/div[4]/div[1]/div[2]/p/a').click()\n",
    "driver.implicitly_wait(3)\n",
    "button = driver.find_element(By.XPATH,r'//*//*[@id=\"content\"]/div/div[2]/div/div/div[2]/user-reviews-summary/div/div[3]/div/div[2]/p/a').click()\n",
    "scroll_down()\n",
    "\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "content = soup.find_all('div', class_='reveal__content')\n",
    "\n",
    "for i in content:\n",
    "    reviews = i.text\n",
    "    reviews = reviews.strip()\n",
    "    print('-', reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32636/1821308300.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reveal__content collapse\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\yukir\\anaconda3\\envs\\crawling\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1806\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   1807\u001b[0m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "k =soup.find('div', class_='review-view-summary row')\n",
    "k2 = k.find('div', class_='col-12')\n",
    "title = k2.find('h1').text\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Mr. Dinky steals Doug! He's a predator!\n",
      "- There is nothing I can say about Doug, good or bad. It's so bland and boring that it makes me question why it's so legendary.\n",
      "- being one of Nickelodeons first original cartoons, it holds up and is a little different from the nick formula, it isn't goofy or silly, it's just about a kid, doug, a rather bland kid i'll admit, but he is an imaginative guy and overall a charming character, while not my absolute favorite character, who is skeeter, doug is still a good character, even if he is a jerk in the disney version.\n",
      "- Realistic and funny. Its been one of my dad’s favorite show from 90s til today. And what I like it’s awesome\n",
      "- There's not much to say besides that Doug is amazing! I've never seen such a show! Too bad Disney ruined it. Skeeter is the best character :)\n"
     ]
    }
   ],
   "source": [
    "content = soup.find_all('div', class_='reveal__content')\n",
    "\n",
    "for i in content:\n",
    "    reviews = i.text\n",
    "    reviews = reviews.strip()\n",
    "    print('-', reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"reveal__content collapse show\" style=\"max-height: 168px;\">\n",
    "    being one of Nickelodeons first original cartoons, it holds up and is a little different from the nick formula, it isn't goofy or silly, it's just about a kid, doug, a rather bland kid i'll admit, but he is an imaginative guy and overall a charming character, while not my absolute favorite character, who is skeeter, doug is still a good character, even if he is a jerk in the disney version.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32636/1844246781.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'review-view-summary row'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mk2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'col-12'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "k =soup.find('div', class_='review-view-summary row')\n",
    "k2 = k.find('div', class_='col-12')\n",
    "title = k2.find('h1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"content\"]/div/div[2]/div/div/div[1]/review-view-summary/div/div[4]/div[1]/div[2]/p/a\n",
    "//*[@id=\"content\"]/div/div[2]/div/div/div[1]/review-view-summary/div/div[4]/div[1]/div[2]/p/a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
