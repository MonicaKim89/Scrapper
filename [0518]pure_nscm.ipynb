{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[0518]pure_nscm.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "45UY1wudwa3Q",
        "yI0_9hOAwp1-",
        "tpEJSmazeDA5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "452e37db8d024034bdafeb32b04fc524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3cddc9d853e42e4b1033aca7579641f",
              "IPY_MODEL_50096f6e55ce4747926bd939dd1e369e",
              "IPY_MODEL_59af566d3a3d4aff84b9ad0af9da858f"
            ],
            "layout": "IPY_MODEL_05e5b7d4b6ba481e854a5c61ff3b6447"
          }
        },
        "e3cddc9d853e42e4b1033aca7579641f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a65c26b3e2b4486395cfa4474da46f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_6207fbf782914b5a891743d74587e157",
            "value": "Downloading: 100%"
          }
        },
        "50096f6e55ce4747926bd939dd1e369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f098c060a94466b982eeb1cc2445d6a",
            "max": 368792146,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be878c342d81421ea5da5c6bcd39ed56",
            "value": 368792146
          }
        },
        "59af566d3a3d4aff84b9ad0af9da858f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f243b5616e964c4b808fc745f090e137",
            "placeholder": "​",
            "style": "IPY_MODEL_b9baa821807d405787f726abfa9624d7",
            "value": " 352M/352M [00:05&lt;00:00, 68.0MB/s]"
          }
        },
        "05e5b7d4b6ba481e854a5c61ff3b6447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65c26b3e2b4486395cfa4474da46f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6207fbf782914b5a891743d74587e157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f098c060a94466b982eeb1cc2445d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be878c342d81421ea5da5c6bcd39ed56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f243b5616e964c4b808fc745f090e137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9baa821807d405787f726abfa9624d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonicaKim89/Text_Mining/blob/main/%5B0518%5Dpure_nscm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g3VM2jeodCs"
      },
      "source": [
        "## HuggingFace 소개\n",
        "![대체 텍스트](https://i.imgur.com/6DM516z.png)  \n",
        "![대체 텍스트](https://i.imgur.com/g6R67h4.png)    \n",
        "https://huggingface.co/transformers/ \n",
        "\n",
        "HuggingFace는 자연어 처리 인공지능 모델에서, BERT 모델 같은 트랜스포머 모델들을 쉽게 다룰 수 있게 해주는 패키지입니다.  \n",
        "기본적으로 pytorch 기반으로 만들어져 있지만, 텐서플로우 2.0에서도 본 패키지를 사용 가능합니다.  \n",
        "텐서플로우 2.0은 기존 케라스를 포함하고 있기 때문에, 기존 텐서플로우나 케라스에 익숙하신 분들이 쉽게 사용할 수 있습니다.  \n",
        "텐서플로우 2.0 기반의 huggingface 사용 방법을 네이버 영화 긍부정 분석을 실습하면서 배워 보도록 하겠습니다.  \n",
        "또한 이번에는 구글의 Multilingual BERT 보다는 한글로 학습된 KOBERT를 활용하여 네이버 감성분석을 해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqNnCd3mwHvC"
      },
      "source": [
        "#목차\n",
        "이번 실습은 <b>1) 네이버 감성분석 데이터 불러오기 및 전처리 2) BERT 인풋 만들기 3) 버트를 활용한 감성분석 모델 만들기 4) 훈련 및 성능 검증 5) 실제 데이터로 실습하기</b>로 구성되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTJEnozuwQpl"
      },
      "source": [
        "# 한글 BERT(KOBERT)를 활용하여 네이버 감성분석 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO3MDrbfpfxV"
      },
      "source": [
        "huggingface 패키지를 Colab에 설치합니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AB7YtQ3rZzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d235af10-36fe-4688-a12f-1dc1586634c4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 85.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 75.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoENjVEcchSP",
        "outputId": "cb253fc4-715c-4f6e-ce99-9a2885cea7b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 3.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0jHL_aR_-V8",
        "outputId": "7c38a086-5e70-45bd-d9b0-32ad6eea8348"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=de09ab0f24463dba11c76bb196f96fac6c25645781fb95e00bd18ceb796e7595\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdvo-EpaqFev"
      },
      "source": [
        "텐서플로우 2와 필요한 모듈들을 임포트합니다.  \n",
        "최근에 텐서플로우 기본 버전은 2로 바뀌었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXvtgWrrdk5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTncidebqMqT"
      },
      "source": [
        "구글 드라이브와 Colab을 연동합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JSqucXTB-jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad68a7a-e1fe-4b91-d478-5935e6709421"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgUYFux4rPVC"
      },
      "source": [
        "이번 예제에서 사용할 네이버 영화 감상분석 데이터를 다운로드 합니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ5lIpQLrOa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c49a649-fca8-4f02-985c-3eb1e19bda2f"
      },
      "source": [
        "# 네이버 영화 감성분석 데이터 다운로드\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 13.92 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YXI6Q-hsQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6814e6f-a57a-4037-b100-0b4020155046"
      },
      "source": [
        "os.listdir('nsmc')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md',\n",
              " 'synopses.json',\n",
              " 'code',\n",
              " 'raw',\n",
              " 'ratings_train.txt',\n",
              " 'ratings.txt',\n",
              " 'ratings_test.txt',\n",
              " '.git']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JFTyGGxwIwI"
      },
      "source": [
        "딥러닝 훈련에 사용 할 train 데이터와 test 데이터를 pandas dataframe 형식으로 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XeWU_2OsRJ3"
      },
      "source": [
        "train = pd.read_table(\"/content/nsmc/ratings_train.txt\")\n",
        "test = pd.read_table(\"/content/nsmc/ratings_test.txt\")"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJpPdIBsVKx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "2aca11f0-f08d-414b-8d9e-9e336bcbc63c"
      },
      "source": [
        "test[50:70]"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                           document  label\n",
              "50   5936280                             돈만있으면 내가 이것보단 더잘만들겠따 ㅋ      0\n",
              "51   6342609                          레이토와 다미앙의 시원한 액션은 어디갔나 ㅁㅊ      0\n",
              "52   9933994   내 참, 이딴 걸 드라마라고.... 그냥 이건 세계보건기구에서 발암물질로 올려야 한다.      0\n",
              "53   8183204  아 극장에서 아무 기대없이 들어갔다가 감탄하면서 보고 나왔는데. 2001년도 였구나...      1\n",
              "54   2590934                                    아이디어가 아주 좋다 재밌다      1\n",
              "55   2077221                                난 재밌던데 평점 왜케 낮지 ``;      1\n",
              "56   7208743                       역사얘기보다 영화배우, 감독들들에게 더 관심이 간다      0\n",
              "57   7926036                     몇년만에 아메리칸조크보고 웃어보는거지. 하아이맛이야바로      1\n",
              "58   5543759                재밌다! 내용도 신선하고 의미도있으며 연기도 좋고 영상도좋다~~      1\n",
              "59   6335957                                 여름이면 한결같이 생각나는 드라마      1\n",
              "60   9745015                                         콩콩~~~~ㅋㄲㅈㅁ      0\n",
              "61   8582196                         ...으...뭐 의도는 좋아봤자 전달이 돼야지;      0\n",
              "62   5347773                              아빠가 낚여서 본적 있다나 슈래기 영화      0\n",
              "63   5494222      \"\"\"결혼은 현실이에요 징징징\"\" 성현아 벗겨놓고 이 정도 얘기 밖에 못 하냐.\"      0\n",
              "64    250638                     엠비씨 무비채널에서 봤는데 또 봐도 엄청 잼나네여 ..      1\n",
              "65   8724656                 어우.. 이게 진짜 무서웠어요?? 진짜로? 시간 아까웠습니다.      0\n",
              "66  10196806                                    몇번을 봐도 볼때마다 재밌다      1\n",
              "67   9868290                          재미있게 잘봤습니다 후속편을 기다리게 만드네요      1\n",
              "68   9690650                      결말 뻔히 알면서 보는데도 너무 슬프고 많이 울었음.      1\n",
              "69   6386772                                    한번본적은업지만재미있을것같다      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-166d55a7-4cb0-4217-8fe2-040d4f0eef27\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>5936280</td>\n",
              "      <td>돈만있으면 내가 이것보단 더잘만들겠따 ㅋ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>6342609</td>\n",
              "      <td>레이토와 다미앙의 시원한 액션은 어디갔나 ㅁㅊ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>9933994</td>\n",
              "      <td>내 참, 이딴 걸 드라마라고.... 그냥 이건 세계보건기구에서 발암물질로 올려야 한다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>8183204</td>\n",
              "      <td>아 극장에서 아무 기대없이 들어갔다가 감탄하면서 보고 나왔는데. 2001년도 였구나...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>2590934</td>\n",
              "      <td>아이디어가 아주 좋다 재밌다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>2077221</td>\n",
              "      <td>난 재밌던데 평점 왜케 낮지 ``;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>7208743</td>\n",
              "      <td>역사얘기보다 영화배우, 감독들들에게 더 관심이 간다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>7926036</td>\n",
              "      <td>몇년만에 아메리칸조크보고 웃어보는거지. 하아이맛이야바로</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>5543759</td>\n",
              "      <td>재밌다! 내용도 신선하고 의미도있으며 연기도 좋고 영상도좋다~~</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>6335957</td>\n",
              "      <td>여름이면 한결같이 생각나는 드라마</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>9745015</td>\n",
              "      <td>콩콩~~~~ㅋㄲㅈㅁ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>8582196</td>\n",
              "      <td>...으...뭐 의도는 좋아봤자 전달이 돼야지;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>5347773</td>\n",
              "      <td>아빠가 낚여서 본적 있다나 슈래기 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>5494222</td>\n",
              "      <td>\"\"\"결혼은 현실이에요 징징징\"\" 성현아 벗겨놓고 이 정도 얘기 밖에 못 하냐.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>250638</td>\n",
              "      <td>엠비씨 무비채널에서 봤는데 또 봐도 엄청 잼나네여 ..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>8724656</td>\n",
              "      <td>어우.. 이게 진짜 무서웠어요?? 진짜로? 시간 아까웠습니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>10196806</td>\n",
              "      <td>몇번을 봐도 볼때마다 재밌다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>9868290</td>\n",
              "      <td>재미있게 잘봤습니다 후속편을 기다리게 만드네요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>9690650</td>\n",
              "      <td>결말 뻔히 알면서 보는데도 너무 슬프고 많이 울었음.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>6386772</td>\n",
              "      <td>한번본적은업지만재미있을것같다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-166d55a7-4cb0-4217-8fe2-040d4f0eef27')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-166d55a7-4cb0-4217-8fe2-040d4f0eef27 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-166d55a7-4cb0-4217-8fe2-040d4f0eef27');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28aO4lGrwMjD"
      },
      "source": [
        "## 버트 인풋 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JGys9DrU4_"
      },
      "source": [
        "한글 데이터를 분석하려면, 100개가 넘는 언어에 대해 훈련된 버트를 사용해야 합니다.  \n",
        "이번에는 한국어 데이터로 훈련되었고, SKT에서 만든 KoBERT를 사용하도록 하겠습니다.  \n",
        "모델을 로드하기에 앞서, 토크나이저를 불러오도록 하겠습니다.  \n",
        "huggingface에서는 아주 쉽게 토크나이저를 불러올 수 있습니다.  \n",
        "https://github.com/monologg/KoBERT-NER 에서 kobert를 tokenize 할 수 있는 코드를 가져왔습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro5nKW78aktM"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWK4I6oC44Or"
      },
      "source": [
        "kobert 토크나이즈를 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Xgl9RtsGul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40236621-1b3e-4f78-afb4-fe3974545f92"
      },
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/monologg/kobert/resolve/main/tokenizer_78b3253a26.model from cache at /root/.cache/huggingface/transformers/7e55d7972628e6fc1babc614b5dd8bb43ab4f9d8541adc9fb1851112a7a7c5cc.4d2f4af7c2ca9df5b147978a95d38840e84801a378eee25756b008638e0bdc7f\n",
            "loading file https://huggingface.co/monologg/kobert/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f\n",
            "loading file https://huggingface.co/monologg/kobert/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/monologg/kobert/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/monologg/kobert/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
            "loading configuration file https://huggingface.co/monologg/kobert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"monologg/kobert\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8002\n",
            "}\n",
            "\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KoBertTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii2W5UbyuN2b"
      },
      "source": [
        "버트를 사용하기에 앞서 가장 기초에 속하는 tokenizer 사용 방법에 대해서 잠시 배워보도록 하겠습니다.  \n",
        "tokenizer.encode => 문장을 버트 모델의 인풋 토큰값으로 바꿔줌  \n",
        "tokenizer.tokenize => 문장을 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eybf2-OluWro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a52cd42-2cf1-457b-aaf9-110a3cb8f8b8"
      },
      "source": [
        "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2366, 5678, 5678, 1192, 1804, 6166, 5760, 3415, 4638, 3272, 3133, 6926, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CshMEDzEcvdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa65e4f-5aa9-4c4c-f8a8-6009bbc74599"
      },
      "source": [
        "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁보는', '내', '내', '▁그대로', '▁들어', '맞', '는', '▁예측', '▁카리스마', '▁없는', '▁악', '역']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORJ-Eq61u2V1"
      },
      "source": [
        "우리가 네이버 영화 평가 긍부정 분석을 위해, train 15만개의 데이터를 버트의 인풋 값으로 바꿔주겠습니다.  \n",
        "버트의 인풋은 토큰, 세그멘트, 마스크로 나눠집니다.  \n",
        "이 세 값이 버트 모형에 들어가서, 버트 모형에 맞게 고차원으로 임베딩이 되게 되는 원리입니다.  \n",
        "\n",
        "토큰은 말 그대로 단어를 단어사전의 위치값으로 표현해주는 것이며, \n",
        "세그멘트는 버트 모형에서 문장이 앞 문장인지, 뒷 문장인지 표현해주는 것입니다.(본 예제는 인풋으로 문장이 하나만 들어가므로 0으로 통일)  \n",
        "마스크는 문장이 유효한 값인지, 아니면 유효하지 않은 값이라 패딩 값으로 채운 것인지를 나타냅니다.  \n",
        "문장이 유효한 값이면 1로 채우고, 유효하지 않은 값이면 0으로 채우게 됩니다.  \n",
        "문장마다 문장 길이는 다르지만, 버트의 인풋 길이는 일정해야 하므로, 버트에서 지정한 문장 길이를 초과하면 패딩값인 0을 채우게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM-XXAuCxIf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b68ad7-4fa0-4e2a-a2c7-0aa5a6b873fd"
      },
      "source": [
        "print(tokenizer.tokenize(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁전', '율을', '▁일으키', '는', '▁영화', '.', '▁다시', '▁보고', '싶', '은', '▁영화']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyhSwEAUxgL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb76ec3-8757-4ed1-b2db-5f3490a41312"
      },
      "source": [
        "print(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4012, 7071, 3815, 5760, 3394, 54, 1574, 2358, 6751, 7086, 3394, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS9ZeYY1yC3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c7801c-973b-4114-b571-aa2e6c72301c"
      },
      "source": [
        "print(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\", max_length=64, pad_to_max_length=True))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4012, 7071, 3815, 5760, 3394, 54, 1574, 2358, 6751, 7086, 3394, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr-l7XNcwtQB"
      },
      "source": [
        "토큰 인풋의 예를 들면 다음과 같습니다.  \n",
        "문장을 토크나이징 하면 \"전율을 일으키는 영화. 다시 보고싶은 영화\"가  \n",
        "\"'전', '##율', '##을', '일', '##으', '##키는', '영화', '.', '다시', '보고', '##싶', '##은', '영화'\" 로 토크나이징이 됩니다.  \n",
        "이거를 버트 인풋에 들어갈 숫자로 바꿔주면,  \n",
        "[\"101, 9665, 119183, 10622, 9641, 119185, 66815, 42428, 119, 25805, 98199, 119088, 10892, 42428, 102\"]  \n",
        "로 바뀌게 됩니다. 여기 나오는 숫자들이 버트 인풋에 들어가는 토큰 인풋입니다.  \n",
        "버트 모형에 들어가는 인풋은 사실 일정한 길이를 가져야 합니다.(본 예제에서는 64)  \n",
        "따라서 남는 부분은 0으로 채워지게 됩니다(패딩)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBa3KLTxyV0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fa052b-d1bc-4d0f-b3bd-3025409d68b4"
      },
      "source": [
        "# 세그멘트 인풋\n",
        "print([0]*64)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQxClepBx5aB"
      },
      "source": [
        "세그멘트 인풋은 문장이 앞문장인지 뒷문장인지 구분해주는 역할을 하는데요  \n",
        "본 문장에서는 문장 하나만 인풋으로 들어가기 때문에 0만 들어가게 되고, 문장 길이만큼의 0이 인풋으로 들어가게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_sprXQ2yczY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2073aeb8-31a0-4c7a-bd48-afe934321388"
      },
      "source": [
        "# 마스크 인풋\n",
        "valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
        "print(valid_num * [1] + (64 - valid_num) * [0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iykc56uIzH1R"
      },
      "source": [
        "마스크 인풋은 토큰 인풋에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 두게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wEKq5qfxH9t"
      },
      "source": [
        "종합하면,  \n",
        "버트의 인풋은 토큰, 세그먼트, 마스크로 이루어져 있습니다.  \n",
        "\"전율을 일으키는 영화. 다시 보고싶은 영화\" 라는 문장을 가지고 예를 들면,\n",
        "\n",
        "토큰 인풋 : [101, 9665, 119183, 10622, 9641, 119185, 66815, 42428, 119, 25805, 98199, 119088, 10892, 42428, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "세그먼트 인풋 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "마스크 인풋 : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LYpgPArzjiI"
      },
      "source": [
        "네이버 영화 평가 문장들을 버트 인풋으로 바꿔보도록 하겠습니다.  \n",
        "문장이 토큰 인풋, 세그먼트 인풋, 마스크 인풋으로 변환 됩니다.  \n",
        "huggingface에서는 순서가 [토큰 인풋, 마스크 인풋, 세그먼트 인풋] 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFFokLO0sj_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb1e855-f98d-46d0-efdc-32901e038150"
      },
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    \n",
        "    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
        "    \n",
        "    tokens, masks, segments, targets = [], [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        # token : 문장을 토큰화함\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], truncation=True, padding='max_length', max_length=SEQ_LEN)\n",
        "       \n",
        "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        \n",
        "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
        "        tokens.append(token)\n",
        "        masks.append(mask)\n",
        "        segments.append(segment)\n",
        "        \n",
        "        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n",
        "        targets.append(data_df[LABEL_COLUMN][i])\n",
        "\n",
        "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    return [tokens, masks, segments], targets\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "SEQ_LEN = 64\n",
        "BATCH_SIZE = 32\n",
        "# 긍부정 문장을 포함하고 있는 칼럼\n",
        "DATA_COLUMN = \"document\"\n",
        "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
        "LABEL_COLUMN = \"label\"\n",
        "\n",
        "# train 데이터를 버트 인풋에 맞게 변환\n",
        "train_x, train_y = load_data(train)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150000/150000 [00:30<00:00, 4848.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ocb17LekZVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a4f7c1-409c-4465-e3a9-20aa832706c1"
      },
      "source": [
        "# 훈련 성능을 검증한 test 데이터를 버트 인풋에 맞게 변환\n",
        "test_x, test_y = load_data(test)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:10<00:00, 4613.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45UY1wudwa3Q"
      },
      "source": [
        "## 버트를 활용한 감성분석 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W85BlYfo2Wge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727,
          "referenced_widgets": [
            "452e37db8d024034bdafeb32b04fc524",
            "e3cddc9d853e42e4b1033aca7579641f",
            "50096f6e55ce4747926bd939dd1e369e",
            "59af566d3a3d4aff84b9ad0af9da858f",
            "05e5b7d4b6ba481e854a5c61ff3b6447",
            "a65c26b3e2b4486395cfa4474da46f0d",
            "6207fbf782914b5a891743d74587e157",
            "7f098c060a94466b982eeb1cc2445d6a",
            "be878c342d81421ea5da5c6bcd39ed56",
            "f243b5616e964c4b808fc745f090e137",
            "b9baa821807d405787f726abfa9624d7"
          ]
        },
        "outputId": "05488680-96c2-4535-f852-9b14456775c5"
      },
      "source": [
        "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
        "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
        "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
        "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
        "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/monologg/kobert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8002\n",
            "}\n",
            "\n",
            "https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphceowxcq\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/352M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "452e37db8d024034bdafeb32b04fc524"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "creating metadata file for /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "loading weights file https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "Loading PyTorch weights from /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "PyTorch checkpoint contains 92,186,880 parameters\n",
            "Loaded 92,186,880 parameters in the TF 2.0 model.\n",
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K52Z9my22mJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d618cb-0fdd-48fa-b9e9-8834f420ec02"
      },
      "source": [
        "bert_outputs"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                                 <KerasTensor: shape=(None, 64, 768) dtype=float32 (created by layer 'tf_bert_model')>),\n",
              "                                                ('pooler_output',\n",
              "                                                 <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model')>)])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmXsWp-x3mII"
      },
      "source": [
        "bert_outputs = bert_outputs[1]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiH25UmlK-R2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a929ca-8714-43c8-b53d-0a8ae0153da3"
      },
      "source": [
        "# Rectified Adam 옵티마이저 사용\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "# 총 batch size * 4 epoch = 2344 * 4\n",
        "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXKy-Jsg3eKA"
      },
      "source": [
        "sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n",
        "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n",
        "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
        "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Si9oO33i70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9a364c-eb51-4784-aef3-bbbf74d4cc56"
      },
      "source": [
        "sentiment_model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " input_masks (InputLayer)       [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " input_segment (InputLayer)     [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  92186880    ['input_word_ids[0][0]',         \n",
            "                                thPoolingAndCrossAt               'input_masks[0][0]',            \n",
            "                                tentions(last_hidde               'input_segment[0][0]']          \n",
            "                                n_state=(None, 64,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 768)          0           ['tf_bert_model[0][1]']          \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            769         ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 92,187,649\n",
            "Trainable params: 92,187,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI0_9hOAwp1-"
      },
      "source": [
        "## 훈련 및 성능 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXuNkLlUbHN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b19458c-8a7f-446b-8f2f-ecc9c37a5ad6"
      },
      "source": [
        "sentiment_model.fit(train_x, train_y, epochs=2, shuffle=True, batch_size=64, validation_data=(test_x, test_y))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2344/2344 [==============================] - 2250s 946ms/step - loss: 0.3760 - accuracy: 0.8124 - val_loss: 0.2847 - val_accuracy: 0.8818\n",
            "Epoch 2/2\n",
            "2344/2344 [==============================] - 2229s 951ms/step - loss: 0.2307 - accuracy: 0.9069 - val_loss: 0.2584 - val_accuracy: 0.8978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe657b5e590>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1Gv3m_34nIX"
      },
      "source": [
        "훈련 모델의 예측 성능을 F1 SCORE로 체크하기 위한 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjv6RMR1jYIe"
      },
      "source": [
        "def predict_convert_data(data_df):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        tokens.append(token)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def predict_load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_x = predict_convert_data(data_df)\n",
        "    return data_x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nyqBoQD4tz1"
      },
      "source": [
        "test 데이터 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzlfYX6xlFEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25334782-06e0-4421-aa14-159c8699c631"
      },
      "source": [
        "test_set = predict_load_data(test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:09<00:00, 5120.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpMD9Ua_7CNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64264c7c-7a39-4bd4-cb8a-dcf640f7aae5"
      },
      "source": [
        "test_set"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[   2,  517, 5515, ...,    1,    1,    1],\n",
              "        [   2,  650,  278, ...,    1,    1,    1],\n",
              "        [   2, 2145, 6844, ...,    1,    1,    1],\n",
              "        ...,\n",
              "        [   2, 1212, 5859, ...,    1,    1,    1],\n",
              "        [   2, 4069, 2420, ...,    1,    1,    1],\n",
              "        [   2, 1914, 5760, ...,    1,    1,    1]]),\n",
              " array([[1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkYBo-dklM8O"
      },
      "source": [
        "preds = sentiment_model.predict(test_set)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ_8XP0ylOe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176476ae-26a1-4f34-a824-94a5b9310f93"
      },
      "source": [
        "# 부정이면 0, 긍정이면 1 출력\n",
        "preds"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9793967 ],\n",
              "       [0.7285405 ],\n",
              "       [0.32048652],\n",
              "       ...,\n",
              "       [0.97137266],\n",
              "       [0.00498554],\n",
              "       [0.12209296]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAJxrViQ4woJ"
      },
      "source": [
        "우리가 훈련한 모델을 F1 SCORE를 바탕으로 성능 측정  \n",
        "F1 SCORE는 precision과 recall을 가중평균하여 계산합니다  \n",
        "recall은 (모델이 TRUE라고 판정한 것의 숫자)/(전체 TRUE의 숫자)  \n",
        "precision은 (진짜 TRUE) / (모델이 TRUE라고 판정한 것의 숫자)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zXTpL9alPml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec477c0-6e47-4685-b131-5c1dfe1b94c0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = test['label']\n",
        "# F1 Score 확인\n",
        "print(classification_report(y_true, np.round(preds,0)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.90      0.90     24827\n",
            "           1       0.90      0.89      0.90     25173\n",
            "\n",
            "    accuracy                           0.90     50000\n",
            "   macro avg       0.90      0.90      0.90     50000\n",
            "weighted avg       0.90      0.90      0.90     50000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03mZ0D7urOl9"
      },
      "source": [
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gVoWj_Mwwhq"
      },
      "source": [
        "# 실제 데이터로 실습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U4-Xd9251W6"
      },
      "source": [
        "문장 하나 하나를 가지고 실제로 분류해보도록 하겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGNnQs0Tlbiv"
      },
      "source": [
        "def sentence_convert_data(data):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "    token = tokenizer.encode(data, max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "    \n",
        "    num_zeros = token.count(0) \n",
        "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros \n",
        "    segment = [0]*SEQ_LEN\n",
        "\n",
        "    tokens.append(token)\n",
        "    segments.append(segment)\n",
        "    masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "def movie_evaluation_predict(sentence):\n",
        "    data_x = sentence_convert_data(sentence)\n",
        "    predict = sentiment_model.predict(data_x)\n",
        "    predict_value = np.ravel(predict)\n",
        "    predict_answer = np.round(predict_value,0).item()\n",
        "    \n",
        "    if predict_answer == 0:\n",
        "      print(\"(부정 확률 : %.2f) 부정적인 영화 평가입니다.\" % (1-predict_value))\n",
        "    elif predict_answer == 1:\n",
        "      print(\"(긍정 확률 : %.2f) 긍정적인 영화 평가입니다.\" % predict_value)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goaRkOc6l8mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb8443b-ac28-4e69-e6ef-6868b5459485"
      },
      "source": [
        "movie_evaluation_predict(\"진료 도 너무 친절하고\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(긍정 확률 : 0.53) 긍정적인 영화 평가입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRpq1b-Zo3Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8862e74-056b-48f6-8237-86b5d7cee194"
      },
      "source": [
        "movie_evaluation_predict(\"대기 시간 발생 엄청 김 이 그거 빼고는\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(부정 확률 : 0.94) 부정적인 영화 평가입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX6aA3NPuVuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bacf179-7b14-4ee5-d11f-d8b632cc5288"
      },
      "source": [
        "movie_evaluation_predict(\"되냐고\")"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(부정 확률 : 0.98) 부정적인 영화 평가입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentiment_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00dHeZjyVNxz",
        "outputId": "4e74caec-e737-4744-e776-53e6566cbeb3"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Projects/derma_sentimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6it22P25S9LP",
        "outputId": "f28f48d2-0c33-4a56-a5a8-0d751ca62068"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Projects/derma_sentimental\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.save('[0518]nsmc_model_pure.h5')"
      ],
      "metadata": {
        "id": "lX01ww5rSf8A"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.save_weights('weight[0518]nsmc_model_pure.h5')"
      ],
      "metadata": {
        "id": "2MPwMcrYTND0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DERMA_TEST"
      ],
      "metadata": {
        "id": "bB6taKiRbe9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델불러오기"
      ],
      "metadata": {
        "id": "tpEJSmazeDA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
        "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
        "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
        "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
        "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])\n",
        "\n",
        "\n",
        "bert_outputs = bert_outputs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_xz8YM2dSfp",
        "outputId": "1fc620ad-b60f-4277-c644-df860c36842e"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/monologg/kobert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "Loading PyTorch weights from /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "PyTorch checkpoint contains 92,186,880 parameters\n",
            "Loaded 92,186,880 parameters in the TF 2.0 model.\n",
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JZoXX7rdSds",
        "outputId": "47bd135f-35ac-4913-a6a4-41586ba5ed17"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n",
        "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n",
        "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
        "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "nIjA1OLTdSbr"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86v3rRRadSZh",
        "outputId": "d8f51e6f-e328-4b27-92c5-77dd174a0fc2"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " input_masks (InputLayer)       [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " input_segment (InputLayer)     [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  92186880    ['input_word_ids[0][0]',         \n",
            "                                thPoolingAndCrossAt               'input_masks[0][0]',            \n",
            "                                tentions(last_hidde               'input_segment[0][0]']          \n",
            "                                n_state=(None, 8, 7                                               \n",
            "                                68),                                                              \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 768)          0           ['tf_bert_model_2[0][1]']        \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            769         ['dropout_113[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 92,187,649\n",
            "Trainable params: 92,187,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.load_weights(\"/content/gdrive/MyDrive/Projects/derma_sentimental/weight[0518]nsmc_model_pure.h5\")"
      ],
      "metadata": {
        "id": "WDIbYPtydSXN"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lqn6aHVPdSVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f9iBzy5vdSTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DERMA TEST"
      ],
      "metadata": {
        "id": "-3GuW9CSVwKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "derma_test = pd.read_excel('/content/gdrive/MyDrive/Projects/Derma/Sentimental Analysis/important_data/[0518]label_test.xlsx', index_col=0)\n",
        "# derma_test.head()"
      ],
      "metadata": {
        "id": "qqNo0VsXSf3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# derma_test = derma_test[['reviews','Unnamed: 3']]\n",
        "derma_test = derma_test.rename(columns=({'reviews':'document'}))\n",
        "derma_test = derma_test.rename(columns=({'Unnamed: 3':'label'}))\n",
        "derma_test['label'] = derma_test['label'].astype(int) \n",
        "derma_test = derma_test.dropna()\n",
        "derma_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "K8M6TlJsXyUx",
        "outputId": "a51d1d99-5c07-47ec-b0ba-8de5f5941e72"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   class_index                                           document  label\n",
              "0            1  갓 구 요 진료 시간 은 1분 정도되엇습니다의사선생님은연세가좀잇으셧고데스크직원분이참...      1\n",
              "2            3                              원장 선생님 은 발음 이 부 정확하시고      0\n",
              "3            4           그리고 진료는 초진 상담 은 간호사 님 이 대화 식으로 이것저것 물어보신      1\n",
              "4            5                                       주변 성신여대의 유명한      1\n",
              "5            1                                ㅜ 원인이나 처치 치료 다 무시하고      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2f85dd0-d845-4898-9067-ac5f2f50934e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_index</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>갓 구 요 진료 시간 은 1분 정도되엇습니다의사선생님은연세가좀잇으셧고데스크직원분이참...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>원장 선생님 은 발음 이 부 정확하시고</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>그리고 진료는 초진 상담 은 간호사 님 이 대화 식으로 이것저것 물어보신</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>주변 성신여대의 유명한</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>ㅜ 원인이나 처치 치료 다 무시하고</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2f85dd0-d845-4898-9067-ac5f2f50934e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2f85dd0-d845-4898-9067-ac5f2f50934e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2f85dd0-d845-4898-9067-ac5f2f50934e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    \n",
        "    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
        "    \n",
        "    tokens, masks, segments, targets = [], [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        # token : 문장을 토큰화함\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], truncation=True, padding='max_length', max_length=SEQ_LEN)\n",
        "       \n",
        "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        \n",
        "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
        "        tokens.append(token)\n",
        "        masks.append(mask)\n",
        "        segments.append(segment)\n",
        "        \n",
        "        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n",
        "        targets.append(data_df[LABEL_COLUMN][i])\n",
        "\n",
        "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    return [tokens, masks, segments], targets\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "    \n",
        "#------------------실제 테스트 할때 바꿀 것--------------------------------\n",
        "SEQ_LEN = 8\n",
        "BATCH_SIZE = 32\n",
        "# 긍부정 문장을 포함하고 있는 칼럼\n",
        "DATA_COLUMN = \"document\"\n",
        "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
        "LABEL_COLUMN = \"label\"\n",
        "\n",
        "def predict_convert_data(data_df):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN].tolist()[i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        tokens.append(token)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def predict_load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_x = predict_convert_data(data_df)\n",
        "    return data_x"
      ],
      "metadata": {
        "id": "25WhiSjjWyfo"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = predict_load_data(derma_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAMoyO2WecWl",
        "outputId": "3c1f2c11-1468-4b99-cdab-01f1d2072f49"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 348/348 [00:00<00:00, 7092.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds=sentiment_model.predict(test_set)"
      ],
      "metadata": {
        "id": "vjHxuyyif06V"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = derma_test['label']\n",
        "# F1 Score 확인\n",
        "print(classification_report(y_true, np.round(preds,0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6fZb46gf7Ul",
        "outputId": "f2ac2304-7b42-4412-cced-d15f584778d5"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.71      0.44       100\n",
            "           1       0.76      0.38      0.51       248\n",
            "\n",
            "    accuracy                           0.47       348\n",
            "   macro avg       0.54      0.54      0.47       348\n",
            "weighted avg       0.64      0.47      0.49       348\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def derma_evaluation_predict(sentence):\n",
        "    # answer_list=[]\n",
        "\n",
        "    data_x = sentence_convert_data(sentence)\n",
        "    predict = sentiment_model.predict(data_x)\n",
        "    predict_value = np.ravel(predict)\n",
        "    predict_answer = np.round(predict_value,0).item()\n",
        "    \n",
        "    if predict_answer == 0:\n",
        "        answer = '부정_'+str(1-predict_value)\n",
        "        # answer_list.append(answer)\n",
        "    elif predict_answer == 1:\n",
        "        answer = '긍정_'+str(predict_value)\n",
        "        # answer_list.append(answer)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "zzG3KcKagHDS"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_list = []\n",
        "\n",
        "for i in derma_test['document'].tolist():\n",
        "    if len(i)>=60:\n",
        "        answer_list.append('해결안됨')\n",
        "    elif len(i)<=60:\n",
        "        answer = derma_evaluation_predict(i)\n",
        "        answer_list.append(answer) \n",
        "            "
      ],
      "metadata": {
        "id": "ER5uQJB0grBF"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "derma_test['pred'] = answer_list\n",
        "derma_test.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "lwTlDWZjiEE_",
        "outputId": "b82c01d4-eaaa-4a98-aac6-023f7f732ef1"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    class_index                                           document  label  \\\n",
              "0             1  갓 구 요 진료 시간 은 1분 정도되엇습니다의사선생님은연세가좀잇으셧고데스크직원분이참...      1   \n",
              "2             3                              원장 선생님 은 발음 이 부 정확하시고      0   \n",
              "3             4           그리고 진료는 초진 상담 은 간호사 님 이 대화 식으로 이것저것 물어보신      1   \n",
              "4             5                                       주변 성신여대의 유명한      1   \n",
              "5             1                                ㅜ 원인이나 처치 치료 다 무시하고      0   \n",
              "6             2                                        예약이나 상담 가능한      1   \n",
              "7             3                                   원장님 이 정체를 알 수 없는      0   \n",
              "8             4                                              친절하시고      1   \n",
              "9             5                                         시설 은 꽤 청결한      1   \n",
              "10            1                 사 마 귀 티 눈 제거 한 줄 평 사마귀 레이저는 진짜 아프다      0   \n",
              "11            2                                  만족 은 하지만 대기 감안하셔요      1   \n",
              "12            3                                   의사 선생님 도 친절하셨습니다      1   \n",
              "13            4                                   하니 그 약 은 도움 이 되지      0   \n",
              "14            5                  합 니 다 시설 깨끗 금액 은 다른 의원 들 이랑 비슷하거나      1   \n",
              "17            3                                    원장님 스타일 이 좀 강하게      0   \n",
              "18            4                                          웃 지만 불친절한      0   \n",
              "19            5                                   실내 인테리어는 깔끔했었습니다      1   \n",
              "22            3                                의사 와 간호사 및 비교 적 저렴한      1   \n",
              "23            4                                             설명해주시며      1   \n",
              "24            5                                 병원 리뷰 병원 시설 은 깨끗하게      1   \n",
              "\n",
              "               pred  \n",
              "0              해결안됨  \n",
              "2   부정_[0.86051214]  \n",
              "3    부정_[0.7720089]  \n",
              "4   긍정_[0.88558835]  \n",
              "5    긍정_[0.7951704]  \n",
              "6   부정_[0.76468086]  \n",
              "7    부정_[0.6424812]  \n",
              "8    부정_[0.6796732]  \n",
              "9    부정_[0.8812556]  \n",
              "10  긍정_[0.71811855]  \n",
              "11   긍정_[0.5230484]  \n",
              "12   부정_[0.7103063]  \n",
              "13   부정_[0.7469664]  \n",
              "14   부정_[0.6747454]  \n",
              "17   부정_[0.7600089]  \n",
              "18   부정_[0.8436253]  \n",
              "19  부정_[0.75104517]  \n",
              "22   긍정_[0.5081326]  \n",
              "23   긍정_[0.8742124]  \n",
              "24   부정_[0.6484231]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c64e1b8-0a1e-47bf-971f-94b0c6b3a14a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_index</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>갓 구 요 진료 시간 은 1분 정도되엇습니다의사선생님은연세가좀잇으셧고데스크직원분이참...</td>\n",
              "      <td>1</td>\n",
              "      <td>해결안됨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>원장 선생님 은 발음 이 부 정확하시고</td>\n",
              "      <td>0</td>\n",
              "      <td>부정_[0.86051214]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>그리고 진료는 초진 상담 은 간호사 님 이 대화 식으로 이것저것 물어보신</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.7720089]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>주변 성신여대의 유명한</td>\n",
              "      <td>1</td>\n",
              "      <td>긍정_[0.88558835]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>ㅜ 원인이나 처치 치료 다 무시하고</td>\n",
              "      <td>0</td>\n",
              "      <td>긍정_[0.7951704]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>예약이나 상담 가능한</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.76468086]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>원장님 이 정체를 알 수 없는</td>\n",
              "      <td>0</td>\n",
              "      <td>부정_[0.6424812]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>친절하시고</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.6796732]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>시설 은 꽤 청결한</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.8812556]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>사 마 귀 티 눈 제거 한 줄 평 사마귀 레이저는 진짜 아프다</td>\n",
              "      <td>0</td>\n",
              "      <td>긍정_[0.71811855]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>만족 은 하지만 대기 감안하셔요</td>\n",
              "      <td>1</td>\n",
              "      <td>긍정_[0.5230484]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>의사 선생님 도 친절하셨습니다</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.7103063]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>하니 그 약 은 도움 이 되지</td>\n",
              "      <td>0</td>\n",
              "      <td>부정_[0.7469664]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>합 니 다 시설 깨끗 금액 은 다른 의원 들 이랑 비슷하거나</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.6747454]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>원장님 스타일 이 좀 강하게</td>\n",
              "      <td>0</td>\n",
              "      <td>부정_[0.7600089]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4</td>\n",
              "      <td>웃 지만 불친절한</td>\n",
              "      <td>0</td>\n",
              "      <td>부정_[0.8436253]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>실내 인테리어는 깔끔했었습니다</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.75104517]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>의사 와 간호사 및 비교 적 저렴한</td>\n",
              "      <td>1</td>\n",
              "      <td>긍정_[0.5081326]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>4</td>\n",
              "      <td>설명해주시며</td>\n",
              "      <td>1</td>\n",
              "      <td>긍정_[0.8742124]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5</td>\n",
              "      <td>병원 리뷰 병원 시설 은 깨끗하게</td>\n",
              "      <td>1</td>\n",
              "      <td>부정_[0.6484231]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c64e1b8-0a1e-47bf-971f-94b0c6b3a14a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c64e1b8-0a1e-47bf-971f-94b0c6b3a14a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c64e1b8-0a1e-47bf-971f-94b0c6b3a14a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NY989UeTiJeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}